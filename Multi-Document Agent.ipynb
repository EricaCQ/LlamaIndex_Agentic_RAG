{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdfe040-bcf7-43c7-a79f-06acf85c78cd",
   "metadata": {},
   "source": [
    "# Multi-Document Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe31d4-b152-48d6-a3ae-4106cbbda4fc",
   "metadata": {},
   "source": [
    "#### This notebook is a reproduction of the experiment4 presented in the Building Agentic RAG with Llamaindex Course\n",
    "\n",
    "#### Source: https://learn.deeplearning.ai/courses/building-agentic-rag-with-llamaindex/lesson/5/building-a-multi-document-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ba1bb-efac-4552-803f-8702623d1c7a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab7cc5e-3e29-4fda-9854-c7121e273ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3f60fd-4bd0-4ef5-baea-79df1b8571c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae031f2e-48cb-4ae9-a8a5-13669115a2e1",
   "metadata": {},
   "source": [
    "### Setup an agent over 3 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a29fa324-fcc9-40e0-bfbe-8a7a2da5aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809717c5-dd8d-4d64-b80a-86a391bfcd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65bb2309-9f13-4582-84fd-797600c0340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0b56f9-da7d-467e-9279-3a4bd928aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cfd7844-1257-4cc6-a758-828f96da5a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554d70cb-c498-4fc1-ac62-e62b9f8cd84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b929bd-5d1e-46e3-9611-62fd7cc78641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets mentioned in the context provided include the RedPajama dataset, PG19 validation set, PG19 test split, LongBench, LEval, book corpus dataset PG19, cleaned Arxiv Math proof-pile dataset, and the proposed dataset SAFECONV designed for evaluating conversation safety.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results across various studies and experiments consistently demonstrate the effectiveness of different attention patterns, such as shifted sparse attention (S2-Attn) and LoRA+, in extending the context window of pre-trained large language models (LLMs) while maintaining comparable performance to full fine-tuning. These methods have shown improved efficiency and reduced computational costs when fine-tuning LLMs to longer context lengths. Additionally, the evaluation results highlight the impact of attention patterns on model performance, with findings indicating the importance of attention architecture during both training and inference stages.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in LongLoRA include the RedPajama dataset, PG19 validation set, PG19 test split, LongBench, LEval, book corpus dataset PG19, cleaned Arxiv Math proof-pile dataset, and the proposed dataset SAFECONV designed for evaluating conversation safety.\n",
      "\n",
      "The evaluation results across various studies and experiments consistently demonstrate the effectiveness of different attention patterns, such as shifted sparse attention (S2-Attn) and LoRA+, in extending the context window of pre-trained large language models (LLMs) while maintaining comparable performance to full fine-tuning. These methods have shown improved efficiency and reduced computational costs when fine-tuning LLMs to longer context lengths. Additionally, the evaluation results highlight the impact of attention patterns on model performance, with findings indicating the importance of attention architecture during both training and inference stages.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de053137-aeb2-4ecb-95cf-1e42939e5ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that utilizes reflection tokens to enhance the quality and factuality of a large language model through a combination of retrieval and self-reflection. It allows the model to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on both the retrieved passages and its own generated content using special tokens called reflection tokens. By incorporating this self-reflective process, Self-RAG aims to improve the overall generation quality, factuality, and verifiability of the language model without compromising its original creativity and versatility.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.0009450649873383732 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59555, Requested 3397. Please try again in 2.952s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6906522338644719 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59473, Requested 3400. Please try again in 2.873s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.945755600482529 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59128, Requested 3587. Please try again in 2.715s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.26241021036085876 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56736, Requested 3796. Please try again in 532ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.1378226392972859 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 57061, Requested 3400. Please try again in 461ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3214186557066263 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59626, Requested 3587. Please try again in 3.213s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3296518005215403 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56654, Requested 3796. Please try again in 450ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "LongLoRA is an efficient method used for extending the context length of Large Language Models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to effectively and efficiently fine-tune models to longer context lengths. This approach allows for extending the context window of LLMs while retaining their original architectures, demonstrating strong empirical results on various tasks. Additionally, LongLoRA is compatible with existing techniques like Flash-Attention2 and is particularly effective when combined with trainable embedding and normalization layers.\n",
      "=== LLM Response ===\n",
      "Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that utilizes reflection tokens to enhance the quality and factuality of a large language model through a combination of retrieval and self-reflection. It allows the model to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on both the retrieved passages and its own generated content using special tokens called reflection tokens. By incorporating this self-reflective process, Self-RAG aims to improve the overall generation quality, factuality, and verifiability of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient method used for extending the context length of Large Language Models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to effectively and efficiently fine-tune models to longer context lengths. This approach allows for extending the context window of LLMs while retaining their original architectures, demonstrating strong empirical results on various tasks. Additionally, LongLoRA is compatible with existing techniques like Flash-Attention2 and is particularly effective when combined with trainable embedding and normalization layers.\n",
      "assistant: Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG:\n",
      "Self-RAG is a framework that utilizes reflection tokens to enhance the quality and factuality of a large language model through a combination of retrieval and self-reflection. It allows the model to adaptively retrieve passages on-demand, generate text informed by these retrieved passages, and reflect on both the retrieved passages and its own generated content using special tokens called reflection tokens. By incorporating this self-reflective process, Self-RAG aims to improve the overall generation quality, factuality, and verifiability of the language model without compromising its original creativity and versatility.\n",
      "\n",
      "2. LongLoRA:\n",
      "LongLoRA is an efficient method used for extending the context length of Large Language Models (LLMs) with limited computation cost. It combines shifted sparse attention (S2-Attn) with LoRA to effectively and efficiently fine-tune models to longer context lengths. This approach allows for extending the context window of LLMs while retaining their original architectures, demonstrating strong empirical results on various tasks. Additionally, LongLoRA is compatible with existing techniques like Flash-Attention2 and is particularly effective when combined with trainable embedding and normalization layers.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59e002-8ade-4503-be2b-1aaade047dc6",
   "metadata": {},
   "source": [
    "## Setup an agent over 11 papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17372c19-f563-4690-8d8e-0e98e1267e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fdb98ea-f343-46a1-ba2d-1b5a4d4ec32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: loftq.pdf\n",
      "Getting tools for paper: swebench.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Getting tools for paper: zipformer.pdf\n",
      "Getting tools for paper: values.pdf\n",
      "Getting tools for paper: finetune_fair_diffusion.pdf\n",
      "Getting tools for paper: knowledge_card.pdf\n",
      "Getting tools for paper: metra.pdf\n",
      "Getting tools for paper: vr_mcl.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdb6af-8ae8-46f9-bef6-bb6b6c2d6d8b",
   "metadata": {},
   "source": [
    "## Extend the Agent with Tool Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06df33dd-fce4-4e70-b9fc-d85483ff3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b483928-5d07-4725-a743-8270e2756e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0357e7-3822-4196-802a-3eae5861aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40db1c3d-441e-4324-8aea-5dbd9c765468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4df3727f-8f28-4535-9c03-fed25da22b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_values', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a794fd09-2b92-4434-b364-f4d35d7c77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever=obj_retriever,\n",
    "    llm=llm, \n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries over a set of given papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b448bee-fbd0-443b-b44e-f6b3509538b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metra with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.3748528395372219 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56247, Requested 3759. Please try again in 6ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.19364296370350298 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56082, Requested 3987. Please try again in 69ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.06395589210831942 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56046, Requested 4010. Please try again in 56ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.09977138826997178 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59660, Requested 4396. Please try again in 4.055s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances drawn from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, the issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated for usability through execution-based validation and are grouped by repository and version. The dataset is continuously updated with new task instances based on pull requests created after the training date of any language model used. Additionally, the dataset involves steps such as applying prediction patches to the codebase, running testing scripts, and determining task completion based on test results. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using different context limits and retrieval settings like BM25 and \"Oracle.\"\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances drawn from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, the issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated for usability through execution-based validation and are grouped by repository and version. The dataset is continuously updated with new task instances based on pull requests created after the training date of any language model used. Additionally, the dataset involves steps such as applying prediction patches to the codebase, running testing scripts, and determining task completion based on test results. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using different context limits and retrieval settings like BM25 and \"Oracle.\"\n",
      "assistant: The evaluation dataset used in MetaGPT is not explicitly mentioned in the provided context information.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances drawn from real GitHub issues and corresponding pull requests across popular Python repositories. It includes task instructions, the issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in the context of software engineering tasks. Task instances are validated for usability through execution-based validation and are grouped by repository and version. The dataset is continuously updated with new task instances based on pull requests created after the training date of any language model used. Additionally, the dataset involves steps such as applying prediction patches to the codebase, running testing scripts, and determining task completion based on test results. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using different context limits and retrieval settings like BM25 and \"Oracle.\"\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "313e84e3-737b-48b2-a06e-e23abae9e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA paper\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5211049228717677 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 56651, Requested 3796. Please try again in 447ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning method for extending the context sizes of pre-trained large language models (LLMs) with limited computational cost. It presents shifted sparse attention (S2-Attn) as a way to effectively enable context extension during fine-tuning, leading to significant computation savings while maintaining performance. The paper combines this with an improved low-rank adaptation (LoRA) approach and demonstrates strong empirical results on various tasks with Llama2 models. LongLoRA allows for extending Llama2 models' context lengths while retaining their original architectures and is compatible with existing techniques like Flash-Attention2. The paper also discusses the importance of learnable embedding and normalization layers in unlocking long context LoRA fine-tuning.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ paper\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.2301177672935023 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59665, Requested 3226. Please try again in 2.891s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 0.48680688053275356 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 59534, Requested 3353. Please try again in 2.887s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "Retrying llama_index.llms.openai.base.OpenAI._achat in 1.070012501528184 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-E5QvRBoFt632vt9JZf6l7XJR on tokens per min (TPM): Limit 60000, Used 57199, Requested 3353. Please try again in 552ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "LoftQ is a novel quantization framework introduced in a paper presented at ICLR 2024. It involves quantizing pre-trained weight matrices and applying low-rank approximations to improve performance in downstream tasks, particularly in low-bit quantization scenarios. LoftQ integrates low-rank approximation with quantization to jointly approximate the original high-precision pre-trained weights, providing a beneficial initialization point for subsequent LoRA fine-tuning. The method has been shown to consistently outperform existing quantization methods, especially in challenging low-bit scenarios, and works effectively with different quantization methods. LoftQ has been evaluated on various tasks such as natural language understanding, question answering, summarization, and natural language generation, demonstrating robustness and improved performance compared to baseline methods like QLoRA and full fine-tuning.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper introduces an efficient fine-tuning method for extending the context sizes of pre-trained large language models (LLMs) with limited computational cost. It presents shifted sparse attention (S2-Attn) as a way to effectively enable context extension during fine-tuning, leading to significant computation savings while maintaining performance. The paper combines this with an improved low-rank adaptation (LoRA) approach and demonstrates strong empirical results on various tasks with Llama2 models. LongLoRA allows for extending Llama2 models' context lengths while retaining their original architectures and is compatible with existing techniques like Flash-Attention2. The paper also discusses the importance of learnable embedding and normalization layers in unlocking long context LoRA fine-tuning.\n",
      "\n",
      "On the other hand, LoftQ is a novel quantization framework introduced in a paper presented at ICLR 2024. It involves quantizing pre-trained weight matrices and applying low-rank approximations to improve performance in downstream tasks, particularly in low-bit quantization scenarios. LoftQ integrates low-rank approximation with quantization to jointly approximate the original high-precision pre-trained weights, providing a beneficial initialization point for subsequent LoRA fine-tuning. The method has been shown to consistently outperform existing quantization methods, especially in challenging low-bit scenarios, and works effectively with different quantization methods. LoftQ has been evaluated on various tasks such as natural language understanding, question answering, summarization, and natural language generation, demonstrating robustness and improved performance compared to baseline methods like QLoRA and full fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57138f2-e526-42d1-94de-a3498a92c81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
